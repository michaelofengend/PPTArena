@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{zhang-etal-2024-pptc,
    title = "{PPTC}-{R} benchmark: Towards Evaluating the Robustness of Large Language Models for {P}ower{P}oint Task Completion",
    author = "Zhang, Zekai  and
      Guo, Yiduo  and
      Liang, Yaobo  and
      Zhao, Dongyan  and
      Duan, Nan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.722/",
    doi = "10.18653/v1/2024.findings-emnlp.722",
    pages = "12387--12402",
    abstract = "The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion-Robustness (PPTC-R) benchmark to measure LLMs' robustness to the user PPT task instruction and software version (Powerpoint). Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance and strong robustness in our benchmark, particularly in the version update and the multilingual settings. However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops. We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM{'}s robustness in task completion and develop more robust LLMs and agents."
}

@inproceedings{guo-etal-2024-pptc,
    title = "{PPTC} Benchmark: Evaluating Large Language Models for {P}ower{P}oint Task Completion",
    author = "Guo, Yiduo  and
      Zhang, Zekai  and
      Liang, Yaobo  and
      Zhao, Dongyan  and
      Duan, Nan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.514/",
    doi = "10.18653/v1/2024.findings-acl.514",
    pages = "8682--8701",
    abstract = "Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1{\%} accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6{\%} session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems ."
}

@inproceedings{zhou2024webarena,
  title     = {WebArena: A Realistic Web Environment for Building Autonomous Agents},
  author    = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
  booktitle = {International Conference on Learning Representations (ICLR)},
  address   = {Vienna, Austria},
  month     = {May},
  year      = {2024},
  url       = {https://arxiv.org/abs/2307.13854}
}

@inproceedings{bonatti2025windowsagentarena,
  title     = {Windows Agent Arena: Evaluating Multi-Modal {OS} Agents at Scale},
  author    = {Bonatti, Rogerio and Zhao, Dan and Bonacci, Francesco and Dupont, Dillon and Abdali, Sara and Li, Yinheng and Lu, Yadong and Wagle, Justin and Koishida, Kazuhito and Bucker, Arthur and Jang, Lawrence Keunho and Hui, Zheng},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {267},
  pages     = {4874--4910},
  publisher = {PMLR},
  month     = {July},
  year      = {2025},
  url       = {https://proceedings.mlr.press/v267/bonatti25a.html}
}

@inproceedings{xie2024osworld,
  title     = {OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments},
  author    = {Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Toh, Jing Hua and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and Liu, Yitao and Xu, Yiheng and Zhou, Shuyan and Savarese, Silvio and Xiong, Caiming and Zhong, Victor and Yu, Tao},
  booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS 2024), Datasets and Benchmarks Track},
  year      = {2024},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets_and_Benchmarks_Track.html}
}

@inproceedings{kapoor2024omniact,
  author    = {Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and AlShikh, Waseem and Salakhutdinov, Ruslan},
  editor    = {Leonardis, Ales and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l},
  title     = {OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web},
  booktitle = {Computer Vision -- ECCV 2024, Proceedings, Part LXVIII},
  series    = {Lecture Notes in Computer Science},
  volume    = {15126},
  pages     = {161--178},
  publisher = {Springer},
  year      = {2024},
  doi       = {10.1007/978-3-031-73113-6_10},
  url       = {https://doi.org/10.1007/978-3-031-73113-6_10}
}

@inproceedings{liu2024agentbench,
  title     = {AgentBench: Evaluating LLMs as Agents},
  author    = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
  booktitle = {Proceedings of the Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=zAdUB0aCTQ}
}

@inproceedings{NEURIPS2023_5950bf29,
  author    = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
  editor    = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  title     = {Mind2Web: Towards a Generalist Agent for the Web},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {36},
  pages     = {28091--28114},
  publisher = {Curran Associates, Inc.},
  year      = {2023},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf}
}

@inproceedings{DBLP:conf/iclr/MialonF0LS24,
  author       = {Mialon, Gr{\'{e}}goire and Fourrier, Cl{\'{e}}mentine and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  title        = {{GAIA:} a benchmark for General {AI} Assistants},
  booktitle    = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=fibxvahvs3},
  biburl       = {https://dblp.org/rec/conf/iclr/MialonF0LS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ge2023openagi,
  title     = {OpenAGI: When LLM Meets Domain Experts},
  author    = {Ge, Yingqiang and Hua, Wenyue and Mei, Kai and Ji, Jianchao and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng},
  booktitle = {Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Datasets and Benchmarks Track},
  year      = {2023},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1190733f217404edc8a7f4e15a57f301-Paper-Datasets_and_Benchmarks.pdf}
}

@misc{hendrycks2025definition,
  title         = {A Definition of AGI},
  author        = {Hendrycks, Dan and Song, Dawn and Szegedy, Christian and Lee, Honglak and Gal, Yarin and Brynjolfsson, Erik and Li, Sharon and Zou, Andy and Levine, Lionel and Salaudeen, Olawale and Hein, Matthias and Zhao, Kevin and Pan, Alexander and Duvenaud, David and Li, Bo and Omohundro, Steve and Alfour, Gabriel and Tegmark, Max and McGrew, Kevin and Marcus, Gary and Tallinn, Jaan and Schmidt, Eric and Bengio, Yoshua and Lee, Kimin and Mazeika, Mantas and Phan, Long and Ingebretsen, George and Khoja, Adam and Xie, Cihang and Han, Bo and Fu, Jie and Liu, Ziwei and Shin, Jinwoo},
  year          = {2025},
  eprint        = {2510.18212},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2510.18212}
}

@misc{moteki2025fieldworkarena,
  title         = {FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks},
  author        = {Moteki, Atsunori and Masui, Shoichi and Yang, Fan and Song, Yueqi and Bisk, Yonatan and Neubig, Graham and Kusajima, Ikuo and Watanabe, Yasuto and Ishida, Hiroyuki and Takahashi, Jun and Jiang, Shan},
  year          = {2025},
  eprint        = {2505.19662},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  doi           = {10.48550/arXiv.2505.19662},
  url           = {https://arxiv.org/abs/2505.19662}
}

@inproceedings{tian-etal-2025-mmina,
  title     = {{MMI}n{A}: Benchmarking Multihop Multimodal {I}nternet Agents},
  author    = {Tian, Shulin and Zhang, Ziniu and Chen, Liangyu and Liu, Ziwei},
  editor    = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
  month     = jul,
  year      = {2025},
  address   = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.findings-acl.703/},
  doi       = {10.18653/v1/2025.findings-acl.703},
  pages     = {13682--13697},
  ISBN      = {979-8-89176-256-5}
}

@inproceedings{koh-etal-2024-visualwebarena,
  title     = {{V}isual{W}eb{A}rena: Evaluating Multimodal Agents on Realistic Visual Web Tasks},
  author    = {Koh, Jing Yu and Lo, Robert and Jang, Lawrence and Duvvur, Vikram and Lim, Ming and Huang, Po-Yu and Neubig, Graham and Zhou, Shuyan and Salakhutdinov, Russ and Fried, Daniel},
  editor    = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.acl-long.50/},
  doi       = {10.18653/v1/2024.acl-long.50},
  pages     = {881--905}
}

@misc{practices2025agenticbenchmarks,
  title         = {Establishing Best Practices for Building Rigorous Agentic Benchmarks},
  author        = {{Benchmarking Practices Authors}},
  year          = {2025},
  eprint        = {2507.02825},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url           = {https://arxiv.org/pdf/2507.02825}
}

@misc{crab2024,
  title         = {CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents},
  author        = {{CRAB Authors}},
  year          = {2024},
  eprint        = {2407.01511},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2407.01511}
}

@misc{kurin2025scalingagents,
  title         = {The Unreasonable Effectiveness of Scaling Agents for Computer Use},
  author        = {{Scaling Agents Authors}},
  year          = {2025},
  eprint        = {2510.02250},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/pdf/2510.02250}
}

@misc{agenticse2025,
  title         = {Agentic Software Engineering: Foundational Pillars and a Research Roadmap},
  author        = {{Agentic Software Engineering Authors}},
  year          = {2025},
  eprint        = {2509.06216},
  archivePrefix = {arXiv},
  primaryClass  = {cs.SE},
  url           = {https://arxiv.org/pdf/2509.06216}
}

@misc{jung2025talkslideslanguagedrivenagents,
  title         = {Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing},
  author        = {Jung, Kyudan and Cho, Hojun and Yun, Jooyeol and Yang, Soyoung and Jang, Jaehyeok and Choo, Jaegul},
  year          = {2025},
  eprint        = {2505.11604},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2505.11604}
}

@misc{ge2025autopresentdesigningstructuredvisuals,
  title         = {AutoPresent: Designing Structured Visuals from Scratch},
  author        = {Ge, Jiaxin and Wang, Zora Zhiruo and Zhou, Xuhui and Peng, Yi-Hao and Subramanian, Sanjay and Tan, Qinyue and Sap, Maarten and Suhr, Alane and Fried, Daniel and Neubig, Graham and Darrell, Trevor},
  year          = {2025},
  eprint        = {2501.00912},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2501.00912}
}

@misc{pptagent2025,
  title         = {PPTAgent: Hierarchical PowerPoint Editing with Vision-Language Agents},
  author        = {{PPTAgent Authors}},
  year          = {2025},
  eprint        = {2501.03936},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2501.03936}
}

@misc{docrefine2025,
  title         = {DocRefine: Multi-Agent Document Editing with Precision Feedback},
  author        = {{DocRefine Authors}},
  year          = {2025},
  eprint        = {2508.07021},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2508.07021v1}
}

@misc{autoslides2025,
  title         = {Auto-Slides: An Interactive Multi-Agent System for Creating and Customizing Research Presentations},
  author        = {{Auto-Slides Authors}},
  year          = {2025},
  eprint        = {2508.10146},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2508.10146}
}

@misc{apivsgui2025,
  title         = {API Agents vs GUI Agents: Evaluating Agent Strategies for Computer Control},
  author        = {{API vs GUI Authors}},
  year          = {2025},
  eprint        = {2503.11069},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2503.11069}
}

@misc{agenticvideoediting2025,
  title         = {Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media},
  author        = {{Agentic Video Editing Authors}},
  year          = {2025},
  eprint        = {2509.16811},
  archivePrefix = {arXiv},
  primaryClass  = {cs.MM},
  url           = {https://arxiv.org/abs/2509.16811}
}

@misc{sheetmind2025,
  title         = {SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation},
  author        = {{SheetMind Authors}},
  year          = {2025},
  eprint        = {2506.12339},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/html/2506.12339v1}
}

@misc{spreadsheetbench2024,
  title         = {SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation},
  author        = {{SpreadsheetBench Authors}},
  year          = {2024},
  eprint        = {2406.14991},
  archivePrefix = {arXiv},
  primaryClass  = {cs.SE},
  url           = {https://arxiv.org/abs/2406.14991}
}

@misc{sheetagentsurvey2024,
  title         = {The Rise of Agentic AI: A Review of Definitions, Frameworks, Architectures, Applications, Evaluation Metrics, and Challenges},
  author        = {{Rise of Agentic AI Survey Authors}},
  year          = {2024},
  note          = {Available via ResearchGate},
  url           = {https://www.researchgate.net/publication/395264831_The_Rise_of_Agentic_AI_A_Review_of_Definitions_Frameworks_Architectures_Applications_Evaluation_Metrics_and_Challenges}
}

@misc{chainofagents2025,
  title         = {Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL},
  author        = {{Chain-of-Agents Authors}},
  year          = {2025},
  eprint        = {2508.13167},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/pdf/2508.13167}
}

@misc{rewardbench2025,
  title         = {Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models},
  author        = {{RewardBench Authors}},
  year          = {2025},
  eprint        = {2502.14191},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/pdf/2502.14191}
}

@misc{bmoca2024,
  title         = {B-MoCa: Benchmarking Mobile Device Control Agents Across Diverse Configurations},
  author        = {{B-MoCa Authors}},
  year          = {2024},
  eprint        = {2404.16660},
  archivePrefix = {arXiv},
  primaryClass  = {cs.HC},
  url           = {https://arxiv.org/abs/2404.16660}
}

@misc{vqartbench2025,
  title         = {VQArt-Bench: A Semantically Rich VQA Benchmark for Art and Cultural Heritage},
  author        = {{VQArt-Bench Authors}},
  year          = {2025},
  eprint        = {2510.12750},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2510.12750}
}

@misc{reasonvqa2025,
  title         = {ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for Visual Question Answering},
  author        = {{ReasonVQA Authors}},
  year          = {2025},
  eprint        = {2507.16403},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2507.16403}
}

@misc{schwenk2022aokvqa,
  title         = {A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge},
  author        = {Schwenk, Dustin and others},
  year          = {2022},
  eprint        = {2206.01718},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2206.01718}
}

@misc{slideaudit2025,
  title         = {SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides},
  author        = {{SlideAudit Authors}},
  year          = {2025},
  eprint        = {2508.03630},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2508.03630}
}

@misc{animationattention2025,
  title         = {Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models},
  author        = {{Animation Needs Attention Authors}},
  year          = {2025},
  eprint        = {2507.03916},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2507.03916}
}

@misc{framesvqa2025,
  title         = {FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering},
  author        = {{FRAMES-VQA Authors}},
  year          = {2025},
  eprint        = {2505.21755},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2505.21755}
}

@misc{cheng2022screenqa,
  title         = {ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots},
  author        = {{ScreenQA Authors}},
  year          = {2022},
  eprint        = {2209.08199},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2209.08199}
}

@misc{yue2023mmmu,
  title         = {MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark},
  author        = {{MMMU Authors}},
  year          = {2023},
  eprint        = {2311.16502},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2311.16502}
}

@misc{mmmupro2024,
  title         = {MMMU-Pro: A More Challenging Benchmark for Multidisciplinary Multimodal Understanding},
  author        = {{MMMU-Pro Authors}},
  year          = {2024},
  eprint        = {2409.02813},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/pdf/2409.02813}
}

@misc{adobe2025expressassistant,
  title         = {Adobe MAX 2025: Express AI Assistant Announcement},
  author        = {{Adobe}},
  year          = {2025},
  note          = {Press release},
  url           = {https://news.adobe.com/news/2025/10/adobe-max-2025-express-ai-assistant}
}

@inproceedings{lee-etal-2024-prometheus,
    title = "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
    author = "Lee, Seongyun  and
      Kim, Seungone  and
      Park, Sue  and
      Kim, Geewook  and
      Seo, Minjoon",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.672/",
    doi = "10.18653/v1/2024.findings-acl.672",
    pages = "11286--11315",
    abstract = "Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model."
}

@misc{xu2024reliablejudge,
  title         = {Is Your Video Language Model a Reliable Judge?},
  author        = {{Reliable Judge Authors}},
  year          = {2024},
  eprint        = {2402.07456},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2402.07456}
}

@misc{zheng2023judge,
  title         = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
  author        = {{MT-Bench Authors}},
  year          = {2023},
  eprint        = {2306.05685},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.05685}
}

@misc{nofreelabels2025,
  title         = {No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding},
  author        = {{No Free Labels Authors}},
  year          = {2025},
  eprint        = {2503.05061},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2503.05061}
}

@misc{whosjudge2025,
  title         = {Who's Your Judge? On the Detectability of LLM-Generated Judgments},
  author        = {{Who's Your Judge Authors}},
  year          = {2025},
  eprint        = {2509.25154},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2509.25154}
}

@misc{surveyllmasjudge2024,
  title         = {A Survey on LLM-as-a-Judge},
  author        = {{Survey on LLM-as-a-Judge Authors}},
  year          = {2024},
  eprint        = {2411.15594},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2411.15594}
}

@misc{agenticframeworks2025,
  title         = {Agentic AI Frameworks: Architectures, Protocols, and Design Challenges},
  author        = {{Agentic AI Frameworks Authors}},
  year          = {2025},
  eprint        = {2508.10146},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2508.10146}
}

@misc{autogen2023,
  title         = {AutoGen: Enabling Next-Gen Large Language Model Applications},
  author        = {{Microsoft}},
  year          = {2023},
  note          = {GitHub repository},
  url           = {https://github.com/microsoft/autogen}
}

@misc{langgraph2024,
  title         = {LangGraph},
  author        = {{LangChain}},
  year          = {2024},
  note          = {GitHub repository},
  url           = {https://github.com/langchain-ai/langgraph}
}

@misc{mllmasajudge2024,
  title         = {MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark},
  author        = {{MLLM-as-a-Judge Authors}},
  year          = {2024},
  url           = {https://arxiv.org/pdf/2402.04788}
}

@misc{webvoyager2025,
  title         = {WebVoyager: A Survey of Web Agents with Multimodal Perception},
  author        = {{WebVoyager Authors}},
  year          = {2025},
  eprint        = {2503.23350},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/html/2503.23350v1}
}

@misc{gur2023webagent,
  title         = {A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis},
  author        = {{WebAgent Authors}},
  year          = {2023},
  eprint        = {2307.12856},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2307.12856}
}

@misc{toolformer2023,
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  author        = {Schick, Timo and others},
  year          = {2023},
  eprint        = {2302.04761},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2302.04761}
}

@misc{gemini2025v25report,
  title         = {Gemini 2.5 Technical Report},
  author        = {{Google DeepMind}},
  year          = {2025},
  note          = {Accessed 2025-10-19},
  url           = {https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf}
}

@misc{openai2025gpt5systemcard,
  title        = {GPT-5 System Card},
  author       = {OpenAI},
  year         = {2025},
  url          = {https://cdn.openai.com/gpt-5-system-card.pdf},
  note         = {Accessed: 2025-10-19}
}

@misc{openai2025chatgptagent,
  title        = {Introducing ChatGPT Agent},
  author       = {OpenAI},
  year         = {2025},
  url          = {https://openai.com/index/introducing-chatgpt-agent/},
  note         = {Accessed: 2025-10-19}
}

@misc{browserarena2025,
  title        = {BrowserArena: Live Evaluation of Web Agents},
  author       = {Anupam, Sagnik and Brown, Davis and Li, Shuo and Wong, Eric and Hassani, Hamed and Bastani, Osbert},
  year         = {2025},
  eprint       = {2510.02418},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI},
  url          = {https://arxiv.org/html/2510.02418v2}
}

@misc{li2024oscopilot,
  title        = {OS-Copilot: Towards Generalist Computer Agents with Self-Improvement},
  author       = {Wu, Zhiyong and Han, Chengcheng and Ding, Zichen and Weng, Zhenmin and Liu, Zhoumianze and Yao, Shunyu and Yu, Tao and Kong, Lingpeng},
  year         = {2024},
  eprint       = {2402.07456},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI},
  url          = {https://arxiv.org/abs/2402.07456}
}

@misc{slidescarnival,
  title        = {SlidesCarnival: Free Presentation Templates},
  author       = {{SlidesCarnival}},
  year         = {2024},
  url          = {https://www.slidescarnival.com/},
  note         = {Online repository of presentation templates}
}

@misc{zenodo,
  title        = {Zenodo: Research Sharing Repository},
  author       = {{European Organization for Nuclear Research}},
  year         = {2024},
  url          = {https://zenodo.org/},
  note         = {Open-access repository for research data and presentations}
}

@misc{slideshare,
  title        = {SlideShare: Professional Content Sharing},
  author       = {{LinkedIn Corporation}},
  year         = {2024},
  url          = {https://www.slideshare.net/},
  note         = {Platform for sharing presentations and professional content}
}

@book{duarte2008slide,
  title        = {slide:ology: The Art and Science of Creating Great Presentations},
  author       = {Duarte, Nancy},
  year         = {2008},
  publisher    = {O'Reilly Media},
  isbn         = {978-0596522346}
}

@book{reynolds2011presentation,
  title        = {Presentation Zen: Simple Ideas on Presentation Design and Delivery},
  author       = {Reynolds, Garr},
  year         = {2011},
  publisher    = {New Riders},
  edition      = {2nd},
  isbn         = {978-0321811981}
}

@book{williams2015non,
  title        = {The Non-Designer's Design Book},
  author       = {Williams, Robin},
  year         = {2015},
  publisher    = {Peachpit Press},
  edition      = {4th},
  isbn         = {978-0133966152}
}

@article{alley2005scientific,
  title        = {The cognitive style of PowerPoint: Pitching out corrupts within},
  author       = {Alley, Michael and Schreiber, Monika and Ramsdell, Kathryn and Muffo, John},
  journal      = {Technical Communication},
  volume       = {52},
  number       = {1},
  pages        = {80--84},
  year         = {2005}
}

@InProceedings{pmlr-v235-maia-polo24a,
  title = 	 {tiny{B}enchmarks: evaluating {LLM}s with fewer examples},
  author =       {Maia Polo, Felipe and Weber, Lucas and Choshen, Leshem and Sun, Yuekai and Xu, Gongjun and Yurochkin, Mikhail},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {34303--34326},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/maia-polo24a/maia-polo24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/maia-polo24a.html},
  abstract = 	 {The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.}
}

@inproceedings{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan},
  booktitle={ICLR},
  year={2022}
}

@misc{pang2025paper2postermultimodalposterautomation,
      title={Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers}, 
      author={Wei Pang and Kevin Qinghong Lin and Xiangru Jian and Xi He and Philip Torr},
      year={2025},
      eprint={2505.21497},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.21497}, 
}

@misc{minimax_m2_2025,
  author       = {MiniMax},
  title        = {MiniMax M2 \& Agent: Ingenious in Simplicity},
  howpublished = {\url{https://www.minimax.io/news/minimax-m2}},
  year         = {2025},
  month        = {October},
  note         = {Accessed: 2025-11-14}
}

@inproceedings{sim-etal-2025-vlms,
    title = "Can {VLM}s Actually See and Read? A Survey on Modality Collapse in Vision-Language Models",
    author = "Sim, Mong Yuan  and
      Zhang, Wei Emma  and
      Dai, Xiang  and
      Fang, Biaoyan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1256/",
    doi = "10.18653/v1/2025.findings-acl.1256",
    pages = "24452--24470",
    ISBN = "979-8-89176-256-5",
    abstract = "Vision-language models (VLMs) integrate textual and visual information, enabling the model to process visual inputs and leverage visual information to generate predictions. Such models are demanding for tasks such as visual question answering, image captioning, and visual grounding. However, some recent work found that VLMs often rely heavily on textual information, ignoring visual information, but are still able to achieve competitive performance in vision-language (VL) tasks. This survey reviews modality collapse analysis work to provide insights into the reason for this unintended behavior. It also reviews probing studies for fine-grained vision-language understanding, presenting current findings on information encoded in VL representations and highlighting potential directions for future research."
}