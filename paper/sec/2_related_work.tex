\section{Related Work}
\label{sec:related}

Agentic presentation editing sits at the intersection of autonomous agent platforms, productivity automation, and evaluation tooling. We review related work highly relevant to PPTArena across multimodal agentic benchmarks, presentation editing systems, industrial agent frameworks, and LLM-as-judge evaluation~\cite{sheetagentsurvey2024,ge2023openagi,kurin2025scalingagents,agenticse2025}.  

\mypar{Multimodal agentic and slide benchmarks.} General-purpose multimodal benchmarks ensure agents possess the perceptual and reasoning depth required for high-quality edits. A line of benchmarks demand grounding and knowledge integration beyond literal reading~\cite{vqartbench2025,reasonvqa2025,schwenk2022aokvqa}. Other datasets stress robustness and preference alignment~\cite{framesvqa2025,rewardbench2025}. More challenging benchmarks aggregate expert-level tasks across multiple disciplines~\cite{yue2023mmmu,mmmupro2024}. PowerPoint-centered benchmarks expose how fragile instruction fidelity remains for today's VLMs. PPTC and PPTC-R evaluates multi-turn editing sessions~\cite{guo-etal-2024-pptc,zhang-etal-2024-pptc}, SlideAudit offers a structured look at design quality, while ANA probes temporal comprehension ignored by static evaluations~\cite{slideaudit2025,animationattention2025}. 

Broader agentic benchmark development has progressed from controlled settings toward realistic, multimodal environment, exposing how fragile action grounding remains~\cite{zhou2024webarena,NEURIPS2023_5950bf29,koh-etal-2024-visualwebarena,browserarena2025}. Operating-system suites extend these ideas to native GUIs and code-driven control~\cite{bonatti2025windowsagentarena,xie2024osworld,kapoor2024omniact}. Newer efforts push beyond desktops into industrial tasks with UI randomization~\cite{moteki2025fieldworkarena,tian-etal-2025-mmina,crab2024,bmoca2024}. Aggregate leaderboards synthesize progress across domains while highlighting how evaluator design, scaling laws, and benchmark rigor influence reported capability~\cite{liu2024agentbench,DBLP:conf/iclr/MialonF0LS24,ge2023openagi,chainofagents2025,practices2025agenticbenchmarks,hendrycks2025definition,kurin2025scalingagents,agenticse2025}. Yet these evaluations rarely interrogate fine-grained document layout proficiency or accessibility compliance. Our PPTArena fills that gap with a productivity-focused suite that couples deterministic XML manifests with dual judge reviews so researchers can attribute errors to planning, perception, or tooling rather than to ambiguous scoring.

\mypar{Presentation editing.} Agentic presentation pipelines blend planning, content synthesis, and low-level manipulation. A line of work explores combinations of generation and refinement, yet each reports brittleness in object targeting, template bias, or cascading errors~\cite{ge2025autopresentdesigningstructuredvisuals,pptagent2025,jung2025talkslideslanguagedrivenagents,docrefine2025,autoslides2025}. Other works highlight the cost of over-reliance on generic templates~\cite{jung2025talkslideslanguagedrivenagents}. Comparative studies confirm API-driven execution outperforms GUI-based approaches for fine-grained control~\cite{apivsgui2025,agenticvideoediting2025}, motivating PPTArena's XML-level enforcement and pixel-grounded targets.

\mypar{Industrial agent and tool-calling.} Progress in agent infrastructure illustrate how self-supervised tools expand computer-use competence~\cite{toolformer2023,gur2023webagent,li2024oscopilot,webvoyager2025} and perception-drive agents~\cite{sheetmind2025,spreadsheetbench2024,cheng2022screenqa}. Open-source orchestration stacks make it easier to compose planners, memory modules, and tool executors, while industrial reports detail production deployments~\cite{autogen2023,langgraph2024,agenticframeworks2025,openai2025chatgptagent,openai2025gpt5systemcard,gemini2025v25report,adobe2025expressassistant}. Concurrent analyses of scaling curves and structured agent engineering~\cite{kurin2025scalingagents,agenticse2025} warn that larger models alone do not guarantee reliability, reinforcing PPTArena's focus on judge audits that make agent improvements interpretable.

\mypar{LLM- and VLM-as-judge evaluation.} Reliable evaluation remains a bottleneck as agent tasks grow more open-ended. A line of work study the feasibility of delegating assessment to specialized VLMs~\cite{xu2024reliablejudge,zheng2023judge,mllmasajudge2024}. Follow-up work exposes risks of judge bias, prompting recommendations such as no-free-label baselines, adversarial judge detection, multi-judge audits, and alignment across heterogeneous tasks~\cite{nofreelabels2025,whosjudge2025,surveyllmasjudge2024,rewardbench2025}. Our work adopts these lessons through a dual-judge protocol, separating instruction compliance from visual and layout grading, ensuring that progress measured on PPTArena reflects genuine improvements rather than exploitation of single-model biases.
